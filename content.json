{"meta":{"title":"庄B","subtitle":"呵呵","description":"什么都没有","author":"Hanszhuang","url":"http://yoursite.com"},"pages":[{"title":"cv","date":"2017-09-01T09:40:59.000Z","updated":"2017-09-01T09:40:59.639Z","comments":true,"path":"cv/index.html","permalink":"http://yoursite.com/cv/index.html","excerpt":"","text":""},{"title":"categories","date":"2017-09-01T07:15:12.000Z","updated":"2017-09-01T07:15:55.432Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-09-01T07:17:37.000Z","updated":"2017-09-01T07:20:07.411Z","comments":false,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"GBDT 调参","slug":"GBDTDemo","date":"2017-09-01T09:00:09.908Z","updated":"2017-09-01T07:24:18.450Z","comments":true,"path":"2017/09/01/GBDTDemo/","link":"","permalink":"http://yoursite.com/2017/09/01/GBDTDemo/","excerpt":"","text":"GBDT类库boosting框架参数 首先，我们来看boosting框架相关的重要参数。由于GradientBoostingClassifier和GradientBoostingRegressor的参数绝大部分相同，我们下面会一起来讲，不同点会单独指出。 n_estimators: 也就是弱学习器的最大迭代次数，或者说最大的弱学习器的个数。一般来说n_estimators太小，容易欠拟合，n_estimators太大，又容易过拟合，一般选择一个适中的数值。默认是100。在实际调参的过程中，我们常常将n_estimators和下面介绍的参数learning_rate一起考虑。 learning_rate: 即每个弱学习器的权重缩减系数vv，也称作步长，在原理篇的正则化章节我们也讲到了，加上了正则化项，我们的强学习器的迭代公式为\\(f_k(x)=f_{k-1}(x)+vhk(x)f_k(x)=f_{k-1}(x)+vhk(x)\\)。vv的取值范围为0&lt;v≤10&lt;v≤1。对于同样的训练集拟合效果，较小的vv意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参。一般来说，可以从一个小一点的vv开始调参，默认是1。 subsample: 即我们在原理篇的正则化章节讲到的子采样，取值为(0,1]。注意这里的子采样和随机森林不一样，随机森林使用的是放回抽样，而这里是不放回抽样。如果取值为1，则全部样本都使用，等于没有使用子采样。如果取值小于1，则只有一部分样本会去做GBDT的决策树拟合。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。推荐在[0.5, 0.8]之间，默认是1.0，即不使用子采样。 init: 即我们的初始化的时候的弱学习器，拟合对应原理篇里面的f0(x)f0(x)，如果不输入，则用训练集样本来做样本集的初始化分类回归预测。否则用init参数提供的学习器做初始化分类回归预测。一般用在我们对数据有先验知识，或者之前做过一些拟合的时候，如果没有的话就不用管这个参数了。 loss: 即我们GBDT算法中的损失函数。分类模型和回归模型的损失函数是不一样的。 1. 对于分类模型，有对数似然损失函数&quot;deviance&quot;和指数损失函数&quot;exponential&quot;两者输入选择。默认是对数似然损失函数&quot;deviance&quot;。在原理篇中对这些分类损失函数有详细的介绍。一般来说，推荐使用默认的&quot;deviance&quot;。它对二元分离和多元分类各自都有比较好的优化。而指数损失函数等于把我们带到了Adaboost算法。 2. 对于回归模型，有均方差&quot;ls&quot;, 绝对损失&quot;lad&quot;, Huber损失&quot;huber&quot;和分位数损失“quantile”。默认是均方差&quot;ls&quot;。一般来说，如果数据的噪音点不多，用默认的均方差&quot;ls&quot;比较好。如果是噪音点较多，则推荐用抗噪音的损失函数&quot;huber&quot;。而如果我们需要对训练集进行分段预测的时候，则采用“quantile”。 alpha：这个参数只有GradientBoostingRegressor有，当我们使用Huber损失“huber”和分位数损失“quantile”时，需要指定分位数的值。默认是0.9，如果噪音点较多，可以适当降低这个分位数的值。 由于GBDT使用了CART回归决策树，因此它的参数基本来源于决策树类，也就是说，和DecisionTreeClassifier和DecisionTreeRegressor的参数基本类似。 划分时考虑的最大特征数max_features: 可以使用很多种类型的值，默认是“None”,意味着划分时考虑所有的特征数；如果是“log2”意味着划分时最多考虑log2Nlog2N个特征；如果是“sqrt”或者“auto”意味着划分时最多考虑N−−√N个特征。如果是整数，代表考虑的特征绝对数。如果是浮点数，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。一般来说，如果样本特征数不多，比如小于50，我们用默认的“None”就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。 决策树最大深度max_depth: 默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间。 内部节点再划分所需最小样本数min_samples_split: 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 叶子节点最少样本数min_samples_leaf: 这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。 叶子节点最小的样本权重和min_weight_fraction_leaf：这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。 最大叶子节点数max_leaf_nodes: 通过限制最大叶子节点数，可以防止过拟合，默认是“None”，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是如果特征分成多的话，可以加以限制，具体的值可以通过交叉验证得到。 节点划分最小不纯度min_impurity_split: 这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。 12345678import pandas as pdimport numpy as npfrom sklearn.ensemble import GradientBoostingClassifierfrom sklearn import cross_validation, metricsfrom sklearn.grid_search import GridSearchCVimport matplotlib.pylab as plt%matplotlib inline C:\\ProgramData\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20. DeprecationWarning) 可以看到类别输出如下，也就是类别0的占大多数。 1234train = pd.read_csv('train_modified.csv')target='Disbursed' # Disbursed的值就是二元分类的输出IDcol = 'ID'train['Disbursed'].value_counts() 0 19680 1 320 Name: Disbursed, dtype: int64 现在我们得到我们的训练集。最后一列Disbursed是分类输出。前面的所有列（不考虑ID列）都是样本特征。 123x_columns = [x for x in train.columns if x not in [target, IDcol]]X = train[x_columns]y = train['Disbursed'] 不管任何参数，都用默认的，我们拟合下数据看看： 123456gbm0 = GradientBoostingClassifier(random_state=10)gbm0.fit(X,y)y_pred = gbm0.predict(X)y_predprob = gbm0.predict_proba(X)[:,1]print \"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred)print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob) Accuracy : 0.9852 AUC Score (Train): 0.900531 首先我们从步长(learning rate)和迭代次数(n_estimators)入手。一般来说,开始选择一个较小的步长来网格搜索最好的迭代次数。这里，我们将步长初始值设置为0.1。对于迭代次数进行网格搜索如下： 123456param_test1 = &#123;'n_estimators':range(20,81,10)&#125;gsearch1 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=300, min_samples_leaf=20,max_depth=8,max_features='sqrt', subsample=0.8,random_state=10), param_grid = param_test1, scoring='roc_auc',iid=False,cv=5)gsearch1.fit(X,y)gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_ ([mean: 0.81285, std: 0.01967, params: {&#39;n_estimators&#39;: 20}, mean: 0.81438, std: 0.01947, params: {&#39;n_estimators&#39;: 30}, mean: 0.81404, std: 0.01970, params: {&#39;n_estimators&#39;: 40}, mean: 0.81593, std: 0.01868, params: {&#39;n_estimators&#39;: 50}, mean: 0.81927, std: 0.01596, params: {&#39;n_estimators&#39;: 60}, mean: 0.81722, std: 0.01750, params: {&#39;n_estimators&#39;: 70}, mean: 0.81485, std: 0.01732, params: {&#39;n_estimators&#39;: 80}], {&#39;n_estimators&#39;: 60}, 0.8192660696138212) 找到了一个合适的迭代次数，现在我们开始对决策树进行调参。首先我们对决策树最大深度max_depth和内部节点再划分所需最小样本数min_samples_split进行网格搜索。 123456param_test2 = &#123;'max_depth':range(3,14,2), 'min_samples_split':range(100,801,200)&#125;gsearch2 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60, min_samples_leaf=20, max_features='sqrt', subsample=0.8, random_state=10), param_grid = param_test2, scoring='roc_auc',iid=False, cv=5)gsearch2.fit(X,y)gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_ ([mean: 0.81199, std: 0.02073, params: {&#39;min_samples_split&#39;: 100, &#39;max_depth&#39;: 3}, mean: 0.81267, std: 0.01985, params: {&#39;min_samples_split&#39;: 300, &#39;max_depth&#39;: 3}, mean: 0.81238, std: 0.01937, params: {&#39;min_samples_split&#39;: 500, &#39;max_depth&#39;: 3}, mean: 0.80925, std: 0.02051, params: {&#39;min_samples_split&#39;: 700, &#39;max_depth&#39;: 3}, mean: 0.81846, std: 0.01843, params: {&#39;min_samples_split&#39;: 100, &#39;max_depth&#39;: 5}, mean: 0.81630, std: 0.01810, params: {&#39;min_samples_split&#39;: 300, &#39;max_depth&#39;: 5}, mean: 0.81315, std: 0.01898, params: {&#39;min_samples_split&#39;: 500, &#39;max_depth&#39;: 5}, mean: 0.81262, std: 0.02090, params: {&#39;min_samples_split&#39;: 700, &#39;max_depth&#39;: 5}, mean: 0.81826, std: 0.02030, params: {&#39;min_samples_split&#39;: 100, &#39;max_depth&#39;: 7}, mean: 0.82137, std: 0.01733, params: {&#39;min_samples_split&#39;: 300, &#39;max_depth&#39;: 7}, mean: 0.81703, std: 0.01773, params: {&#39;min_samples_split&#39;: 500, &#39;max_depth&#39;: 7}, mean: 0.81383, std: 0.02327, params: {&#39;min_samples_split&#39;: 700, &#39;max_depth&#39;: 7}, mean: 0.81094, std: 0.02178, params: {&#39;min_samples_split&#39;: 100, &#39;max_depth&#39;: 9}, mean: 0.80968, std: 0.02622, params: {&#39;min_samples_split&#39;: 300, &#39;max_depth&#39;: 9}, mean: 0.81476, std: 0.01973, params: {&#39;min_samples_split&#39;: 500, &#39;max_depth&#39;: 9}, mean: 0.81601, std: 0.02576, params: {&#39;min_samples_split&#39;: 700, &#39;max_depth&#39;: 9}, mean: 0.81330, std: 0.02215, params: {&#39;min_samples_split&#39;: 100, &#39;max_depth&#39;: 11}, mean: 0.81309, std: 0.02696, params: {&#39;min_samples_split&#39;: 300, &#39;max_depth&#39;: 11}, mean: 0.81694, std: 0.02397, params: {&#39;min_samples_split&#39;: 500, &#39;max_depth&#39;: 11}, mean: 0.81347, std: 0.02702, params: {&#39;min_samples_split&#39;: 700, &#39;max_depth&#39;: 11}, mean: 0.81481, std: 0.01778, params: {&#39;min_samples_split&#39;: 100, &#39;max_depth&#39;: 13}, mean: 0.80912, std: 0.02153, params: {&#39;min_samples_split&#39;: 300, &#39;max_depth&#39;: 13}, mean: 0.81959, std: 0.01654, params: {&#39;min_samples_split&#39;: 500, &#39;max_depth&#39;: 13}, mean: 0.81382, std: 0.02258, params: {&#39;min_samples_split&#39;: 700, &#39;max_depth&#39;: 13}], {&#39;max_depth&#39;: 7, &#39;min_samples_split&#39;: 300}, 0.8213724275914632) 由于决策树深度7是一个比较合理的值，我们把它定下来，对于内部节点再划分所需最小样本数min_samples_split，我们暂时不能一起定下来，因为这个还和决策树其他的参数存在关联。下面我们再对内部节点再划分所需最小样本数min_samples_split和叶子节点最少样本数min_samples_leaf一起调参。 123456param_test3 = &#123;'min_samples_split':range(800,1900,200), 'min_samples_leaf':range(60,101,10)&#125;gsearch3 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7, max_features='sqrt', subsample=0.8, random_state=10), param_grid = param_test3, scoring='roc_auc',iid=False, cv=5)gsearch3.fit(X,y)gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_ ([mean: 0.81828, std: 0.02251, params: {&#39;min_samples_split&#39;: 800, &#39;min_samples_leaf&#39;: 60}, mean: 0.81731, std: 0.02344, params: {&#39;min_samples_split&#39;: 1000, &#39;min_samples_leaf&#39;: 60}, mean: 0.82220, std: 0.02250, params: {&#39;min_samples_split&#39;: 1200, &#39;min_samples_leaf&#39;: 60}, mean: 0.81447, std: 0.02125, params: {&#39;min_samples_split&#39;: 1400, &#39;min_samples_leaf&#39;: 60}, mean: 0.81495, std: 0.01626, params: {&#39;min_samples_split&#39;: 1600, &#39;min_samples_leaf&#39;: 60}, mean: 0.81528, std: 0.02140, params: {&#39;min_samples_split&#39;: 1800, &#39;min_samples_leaf&#39;: 60}, mean: 0.81590, std: 0.02517, params: {&#39;min_samples_split&#39;: 800, &#39;min_samples_leaf&#39;: 70}, mean: 0.81573, std: 0.02207, params: {&#39;min_samples_split&#39;: 1000, &#39;min_samples_leaf&#39;: 70}, mean: 0.82021, std: 0.02521, params: {&#39;min_samples_split&#39;: 1200, &#39;min_samples_leaf&#39;: 70}, mean: 0.81512, std: 0.01995, params: {&#39;min_samples_split&#39;: 1400, &#39;min_samples_leaf&#39;: 70}, mean: 0.81395, std: 0.02081, params: {&#39;min_samples_split&#39;: 1600, &#39;min_samples_leaf&#39;: 70}, mean: 0.81587, std: 0.02082, params: {&#39;min_samples_split&#39;: 1800, &#39;min_samples_leaf&#39;: 70}, mean: 0.82064, std: 0.02698, params: {&#39;min_samples_split&#39;: 800, &#39;min_samples_leaf&#39;: 80}, mean: 0.81490, std: 0.02475, params: {&#39;min_samples_split&#39;: 1000, &#39;min_samples_leaf&#39;: 80}, mean: 0.82009, std: 0.02568, params: {&#39;min_samples_split&#39;: 1200, &#39;min_samples_leaf&#39;: 80}, mean: 0.81850, std: 0.02226, params: {&#39;min_samples_split&#39;: 1400, &#39;min_samples_leaf&#39;: 80}, mean: 0.81855, std: 0.02099, params: {&#39;min_samples_split&#39;: 1600, &#39;min_samples_leaf&#39;: 80}, mean: 0.81666, std: 0.02249, params: {&#39;min_samples_split&#39;: 1800, &#39;min_samples_leaf&#39;: 80}, mean: 0.81960, std: 0.02437, params: {&#39;min_samples_split&#39;: 800, &#39;min_samples_leaf&#39;: 90}, mean: 0.81560, std: 0.02235, params: {&#39;min_samples_split&#39;: 1000, &#39;min_samples_leaf&#39;: 90}, mean: 0.81936, std: 0.02542, params: {&#39;min_samples_split&#39;: 1200, &#39;min_samples_leaf&#39;: 90}, mean: 0.81362, std: 0.02254, params: {&#39;min_samples_split&#39;: 1400, &#39;min_samples_leaf&#39;: 90}, mean: 0.81429, std: 0.02417, params: {&#39;min_samples_split&#39;: 1600, &#39;min_samples_leaf&#39;: 90}, mean: 0.81299, std: 0.02262, params: {&#39;min_samples_split&#39;: 1800, &#39;min_samples_leaf&#39;: 90}, mean: 0.82000, std: 0.02511, params: {&#39;min_samples_split&#39;: 800, &#39;min_samples_leaf&#39;: 100}, mean: 0.82209, std: 0.01816, params: {&#39;min_samples_split&#39;: 1000, &#39;min_samples_leaf&#39;: 100}, mean: 0.81821, std: 0.02337, params: {&#39;min_samples_split&#39;: 1200, &#39;min_samples_leaf&#39;: 100}, mean: 0.81922, std: 0.02377, params: {&#39;min_samples_split&#39;: 1400, &#39;min_samples_leaf&#39;: 100}, mean: 0.81545, std: 0.02221, params: {&#39;min_samples_split&#39;: 1600, &#39;min_samples_leaf&#39;: 100}, mean: 0.81704, std: 0.02509, params: {&#39;min_samples_split&#39;: 1800, &#39;min_samples_leaf&#39;: 100}], {&#39;min_samples_leaf&#39;: 60, &#39;min_samples_split&#39;: 1200}, 0.8222032996697154) 我们调了这么多参数了，终于可以都放到GBDT类里面去看看效果了。现在我们用新参数拟合数据： 1234567gbm1 = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7, min_samples_leaf =60, min_samples_split =1200, max_features='sqrt', subsample=0.8, random_state=10)gbm1.fit(X,y)y_pred = gbm1.predict(X)y_predprob = gbm1.predict_proba(X)[:,1]print \"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred)print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob) Accuracy : 0.984 AUC Score (Train): 0.908099 对比我们最开始完全不调参的拟合效果，可见精确度稍有下降，主要原理是我们使用了0.8的子采样，20%的数据没有参与拟合。 现在我们再对最大特征数max_features进行网格搜索。 123456param_test4 = &#123;'max_features':range(7,20,2)&#125;gsearch4 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7, min_samples_leaf =60, min_samples_split =1200, subsample=0.8, random_state=10), param_grid = param_test4, scoring='roc_auc',iid=False, cv=5)gsearch4.fit(X,y)gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_ ([mean: 0.82220, std: 0.02250, params: {&#39;max_features&#39;: 7}, mean: 0.82241, std: 0.02421, params: {&#39;max_features&#39;: 9}, mean: 0.82108, std: 0.02302, params: {&#39;max_features&#39;: 11}, mean: 0.82064, std: 0.01900, params: {&#39;max_features&#39;: 13}, mean: 0.82198, std: 0.01514, params: {&#39;max_features&#39;: 15}, mean: 0.81355, std: 0.02053, params: {&#39;max_features&#39;: 17}, mean: 0.81877, std: 0.01863, params: {&#39;max_features&#39;: 19}], {&#39;max_features&#39;: 9}, 0.822412506351626) 现在我们再对子采样的比例进行网格搜索： 123456param_test5 = &#123;'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]&#125;gsearch5 = GridSearchCV(estimator = GradientBoostingClassifier(learning_rate=0.1, n_estimators=60,max_depth=7, min_samples_leaf =60, min_samples_split =1200, max_features=9, random_state=10), param_grid = param_test5, scoring='roc_auc',iid=False, cv=5)gsearch5.fit(X,y)gsearch5.grid_scores_, gsearch5.best_params_, gsearch5.best_score_ ([mean: 0.81828, std: 0.02392, params: {&#39;subsample&#39;: 0.6}, mean: 0.82344, std: 0.02708, params: {&#39;subsample&#39;: 0.7}, mean: 0.81673, std: 0.02196, params: {&#39;subsample&#39;: 0.75}, mean: 0.82241, std: 0.02421, params: {&#39;subsample&#39;: 0.8}, mean: 0.82285, std: 0.02446, params: {&#39;subsample&#39;: 0.85}, mean: 0.81738, std: 0.02236, params: {&#39;subsample&#39;: 0.9}], {&#39;subsample&#39;: 0.7}, 0.8234378969766262) 现在我们基本已经得到我们所有调优的参数结果了。这时我们可以减半步长，最大迭代次数加倍来增加我们模型的泛化能力。再次拟合我们的模型： 1234567gbm2 = GradientBoostingClassifier(learning_rate=0.05, n_estimators=120,max_depth=7, min_samples_leaf =60, min_samples_split =1200, max_features=9, subsample=0.7, random_state=10)gbm2.fit(X,y)y_pred = gbm2.predict(X)y_predprob = gbm2.predict_proba(X)[:,1]print \"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred)print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob) Accuracy : 0.984 AUC Score (Train): 0.905324 可以看到AUC分数比起之前的版本稍有下降，这个原因是我们为了增加模型泛化能力，为防止过拟合而减半步长，最大迭代次数加倍，同时减小了子采样的比例，从而减少了训练集的拟合程度。 下面我们继续将步长缩小5倍，最大迭代次数增加5倍，继续拟合我们的模型： 1234567gbm3 = GradientBoostingClassifier(learning_rate=0.01, n_estimators=600,max_depth=7, min_samples_leaf =60, min_samples_split =1200, max_features=9, subsample=0.7, random_state=10)gbm3.fit(X,y)y_pred = gbm3.predict(X)y_predprob = gbm3.predict_proba(X)[:,1]print \"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred)print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob) Accuracy : 0.984 AUC Score (Train): 0.908581 可见减小步长增加迭代次数可以在保证泛化能力的基础上增加一些拟合程度。 最后我们继续步长缩小一半，最大迭代次数增加2倍，拟合我们的模型： 1234567gbm4 = GradientBoostingClassifier(learning_rate=0.005, n_estimators=1200,max_depth=7, min_samples_leaf =60, min_samples_split =1200, max_features=9, subsample=0.7, random_state=10)gbm4.fit(X,y)y_pred = gbm4.predict(X)y_predprob = gbm4.predict_proba(X)[:,1]print \"Accuracy : %.4g\" % metrics.accuracy_score(y.values, y_pred)print \"AUC Score (Train): %f\" % metrics.roc_auc_score(y, y_predprob) Accuracy : 0.984 AUC Score (Train): 0.908232 此时由于步长实在太小，导致拟合效果反而变差，也就是说，步长不能设置的过小 12","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"XGBOOST 调参","slug":"XGBoostDemo","date":"2017-09-01T08:59:50.180Z","updated":"2017-09-01T09:00:39.485Z","comments":true,"path":"2017/09/01/XGBoostDemo/","link":"","permalink":"http://yoursite.com/2017/09/01/XGBoostDemo/","excerpt":"","text":"XGBOOST的参数 1. 通用参数：宏观函数控制。 2. Booster参数：控制每一步的booster(tree/regression)。 3. 学习目标参数：控制训练目标的表现。 通用参数 booster[默认gbtree] 选择每次迭代的模型，有两种选择： gbtree：基于树的模型 gbliner：线性模型 silent[默认0] 当这个参数值为1时，静默模式开启，不会输出任何信息。 一般这个参数就保持默认的0，因为这样能帮我们更好地理解模型。 nthread[默认值为最大可能的线程数] 这个参数用来进行多线程控制，应当输入系统的核数。 如果你希望使用CPU全部的核，那就不要输入这个参数，算法会自动检测它。 还有两个参数，XGBoost会自动设置，目前你不用管它。接下来咱们一起看booster参数。 booster参数 尽管有两种booster可供选择，我这里只介绍tree booster，因为它的表现远远胜过linear booster，所以linear booster很少用到。 eta[默认0.3] 和GBM中的 learning rate 参数类似。 通过减少每一步的权重，可以提高模型的鲁棒性。 典型值为0.01-0.2。 min_child_weight[默认1] 决定最小叶子节点样本权重和。 和GBM的 min_child_leaf 参数类似，但不完全一样。XGBoost的这个参数是最小样本权重的和，而GBM参数是最小样本总数。 这个参数用于避免过拟合。当它的值较大时，可以避免模型学习到局部的特殊样本。 但是如果这个值过高，会导致欠拟合。这个参数需要使用CV来调整。 max_depth[默认6] 和GBM中的参数相同，这个值为树的最大深度。 这个值也是用来避免过拟合的。max_depth越大，模型会学到更具体更局部的样本。 需要使用CV函数来进行调优。 典型值：3-10 max_leaf_nodes 树上最大的节点或叶子的数量。 可以替代max_depth的作用。因为如果生成的是二叉树，一个深度为n的树最多生成n2个叶子。 如果定义了这个参数，GBM会忽略max_depth参数。 gamma[默认0] 在节点分裂时，只有分裂后损失函数的值下降了，才会分裂这个节点。Gamma指定了节点分裂所需的最小损失函数下降值。 这个参数的值越大，算法越保守。这个参数的值和损失函数息息相关，所以是需要调整的。 max_delta_step[默认0] 这参数限制每棵树权重改变的最大步长。如果这个参数的值为0，那就意味着没有约束。如果它被赋予了某个正值，那么它会让这个算法更加保守。 通常，这个参数不需要设置。但是当各类别的样本十分不平衡时，它对逻辑回归是很有帮助的。 这个参数一般用不到，但是你可以挖掘出来它更多的用处。 subsample[默认1] 和GBM中的subsample参数一模一样。这个参数控制对于每棵树，随机采样的比例。 减小这个参数的值，算法会更加保守，避免过拟合。但是，如果这个值设置得过小，它可能会导致欠拟合。 典型值：0.5-1 colsample_bytree[默认1] 和GBM里面的max_features参数类似。用来控制每棵随机采样的列数的占比(每一列是一个特征)。 典型值：0.5-1 colsample_bylevel[默认1] 用来控制树的每一级的每一次分裂，对列数的采样的占比。 我个人一般不太用这个参数，因为subsample参数和colsample_bytree参数可以起到相同的作用。但是如果感兴趣，可以挖掘这个参数更多的用处。 lambda[默认1] 权重的L2正则化项。(和Ridge regression类似)。 这个参数是用来控制XGBoost的正则化部分的。虽然大部分数据科学家很少用到这个参数，但是这个参数在减少过拟合上还是可以挖掘出更多用处的。 alpha[默认1] 权重的L1正则化项。(和Lasso regression类似)。 可以应用在很高维度的情况下，使得算法的速度更快。 cale_pos_weight[默认1] 在各类别样本十分不平衡时，把这个参数设定为一个正值，可以使算法更快收敛。 学习目标参数 这个参数用来控制理想的优化目标和每一步结果的度量方法。 objective[默认reg:linear] 这个参数定义需要被最小化的损失函数。最常用的值有： binary:logistic 二分类的逻辑回归，返回预测的概率(不是类别)。 multi:softmax 使用softmax的多分类器，返回预测的类别(不是概率)。 在这种情况下，你还需要多设一个参数：num_class(类别数目)。 multi:softprob 和multi:softmax参数一样，但是返回的是每个数据属于各个类别的概率。 eval_metric[默认值取决于objective参数的取值] 对于有效数据的度量方法。 对于回归问题，默认值是rmse，对于分类问题，默认值是error。 典型值有： rmse 均方根误差\\((\\sqrt{\\frac{\\sum_{i=1}^{N}\\epsilon^{2}}{N}})\\) mae 平均绝对误差\\((\\frac{\\sum_{i=1}^{N}\\left | \\epsilon \\right |}{N})\\) logloss 负对数似然函数值 error 二分类错误率(阈值为0.5) merror 多分类错误率 mlogloss 多分类 logloss损失函数 auc 曲线下面积 seed(默认0) 随机数的种子 设置它可以复现随机数据的结果，也可以用于调整参数 如果你之前用的是Scikit-learn,你可能不太熟悉这些参数。但是有个好消息，python的XGBoost模块有一个sklearn包，XGBClassifier。这个包中的参数是按sklearn风格命名的。会改变的函数名是： 1. #### eta -&gt;learning_rate 2. #### lambda-&gt;reg_lambda 3. #### alpha-&gt;reg_alpha 你肯定在疑惑为啥咱们没有介绍和GBM中的’n_estimators’类似的参数。XGBClassifier中确实有一个类似的参数，但是，是在标准XGBoost实现中调用拟合函数时，把它作为’num_boosting_rounds’参数传入。 12345678910111213141516#Import libraries:import pandas as pdimport numpy as npimport xgboost as xgbfrom xgboost.sklearn import XGBClassifierfrom sklearn import cross_validation, metrics #Additional scklearn functionsfrom sklearn.grid_search import GridSearchCV #Perforing grid searchimport matplotlib.pylab as plt%matplotlib inlinefrom matplotlib.pylab import rcParamsrcParams['figure.figsize'] = 12, 4train = pd.read_csv('train_modified_xgb.csv')target = 'Disbursed'IDcol = 'ID' 注意我import了两种XGBoost： xgb - 直接引用xgboost。接下来会用到其中的“cv”函数。 XGBClassifier - 是xgboost的sklearn包。这个包允许我们像GBM一样使用Grid Search 和并行处理。 在向下进行之前，我们先定义一个函数，它可以帮助我们建立XGBoost models 并进行交叉验证。好消息是你可以直接用下面的函数，以后再自己的models中也可以使用它。 1234567891011121314151617181920212223def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50): if useTrainCV: xgb_param = alg.get_xgb_params() xgtrain = xgb.DMatrix(dtrain[predictors].values, label=dtrain[target].values) cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'],show_stdv=False, nfold=cv_folds,metrics='auc', early_stopping_rounds=early_stopping_rounds) alg.set_params(n_estimators=cvresult.shape[0]) #Fit the algorithm on the data alg.fit(dtrain[predictors], dtrain['Disbursed'],eval_metric='auc') #Predict training set: dtrain_predictions = alg.predict(dtrain[predictors]) dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1] #Print model report: print \"\\nModel Report\" print \"Accuracy : %.4g\" % metrics.accuracy_score(dtrain['Disbursed'].values, dtrain_predictions) print \"AUC Score (Train): %f\" % metrics.roc_auc_score(dtrain['Disbursed'], dtrain_predprob) feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False) feat_imp.plot(kind='bar', title='Feature Importances') plt.ylabel('Feature Importance Score') 这个函数和GBM中使用的有些许不同。不过本文章的重点是讲解重要的概念，而不是写代码。如果哪里有不理解的地方，请在下面评论，不要有压力。注意xgboost的sklearn包没有“feature_importance”这个量度，但是get_fscore()函数有相同的功能。 参数调优的一般方法 我们会使用和GBM中相似的方法。需要进行如下步骤： 选择较高的学习速率(learning rate)。一般情况下，学习速率的值为0.1。但是，对于不同的问题，理想的学习速率有时候会在0.05到0.3之间波动。选择对应于此学习速率的理想决策树数量。XGBoost有一个很有用的函数“cv”，这个函数可以在每一次迭代中使用交叉验证，并返回理想的决策树数量。 对于给定的学习速率和决策树数量，进行决策树特定参数调优(max_depth, min_child_weight, gamma, subsample, colsample_bytree)。在确定一棵树的过程中，我们可以选择不同的参数，待会儿我会举例说明。 xgboost的正则化参数的调优。(lambda, alpha)。这些参数可以降低模型的复杂度，从而提高模型的表现。 降低学习速率，确定理想参数。 咱们一起详细地一步步进行这些操作。 第一步：确定学习速率和tree_based 参数调优的估计器数目 为了确定boosting参数，我们要先给其它参数一个初始值。咱们先按如下方法取值： max_depth = 5 :这个参数的取值最好在3-10之间。我选的起始值为5，但是你也可以选择其它的值。起始值在4-6之间都是不错的选择。 min_child_weight = 1:在这里选了一个比较小的值，因为这是一个极不平衡的分类问题。因此，某些叶子节点下的值会比较小。 gamma = 0: 起始值也可以选其它比较小的值，在0.1到0.2之间就可以。这个参数后继也是要调整的。 subsample, colsample_bytree = 0.8: 这个是最常见的初始值了。典型值的范围在0.5-0.9之间。 scale_pos_weight = 1: 这个值是因为类别十分不平衡。 注意哦，上面这些参数的值只是一个初始的估计值，后继需要调优。这里把学习速率就设成默认的0.1。然后用xgboost中的cv函数来确定最佳的决策树数量。前文中的函数可以完成这个工作。 123456789101112131415#Choose all predictors except target &amp; IDcolspredictors = [x for x in train.columns if x not in [target, IDcol]]xgb1 = XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)modelfit(xgb1, train, predictors) Model Report Accuracy : 0.9854 AUC Score (Train): 0.891681 png 从输出结果可以看出，在学习速率为0.1时，理想的决策树数目是140。这个数字对你而言可能比较高，当然这也取决于你的系统的性能。 第二步： max_depth 和 min_weight 参数调优 我们先对这两个参数调优，是因为它们对最终结果有很大的影响。首先，我们先大范围地粗调参数，然后再小范围地微调。 注意：在这一节我会进行高负荷的栅格搜索(grid search)，这个过程大约需要15-30分钟甚至更久，具体取决于你系统的性能。你也可以根据自己系统的性能选择不同的值。 12345678910param_test1 = &#123; 'max_depth':range(3,10,2), 'min_child_weight':range(1,6,2)&#125;gsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=5,min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27), param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)gsearch1.fit(train[predictors],train[target])gsearch1.grid_scores_, gsearch1.best_params_, gsearch1.best_score_ ([mean: 0.83764, std: 0.00875, params: {&#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 1}, mean: 0.83837, std: 0.00825, params: {&#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 3}, mean: 0.83716, std: 0.00818, params: {&#39;max_depth&#39;: 3, &#39;min_child_weight&#39;: 5}, mean: 0.84016, std: 0.00680, params: {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 1}, mean: 0.83965, std: 0.00537, params: {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 3}, mean: 0.83935, std: 0.00548, params: {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 5}, mean: 0.83570, std: 0.00587, params: {&#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 1}, mean: 0.83448, std: 0.00726, params: {&#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 3}, mean: 0.83456, std: 0.00554, params: {&#39;max_depth&#39;: 7, &#39;min_child_weight&#39;: 5}, mean: 0.82851, std: 0.00651, params: {&#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 1}, mean: 0.82955, std: 0.00580, params: {&#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 3}, mean: 0.83158, std: 0.00677, params: {&#39;max_depth&#39;: 9, &#39;min_child_weight&#39;: 5}], {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 1}, 0.8401574515811714) 至此，我们对于数值进行了较大跨度的12中不同的排列组合，可以看出理想的max_depth值为5，理想的min_child_weight值为5。在这个值附近我们可以再进一步调整，来找出理想值。我们把上下范围各拓展1，因为之前我们进行组合的时候，参数调整的步长是2。 12345678910param_test2 = &#123; 'max_depth':[4,5,6], 'min_child_weight':[4,5,6]&#125;gsearch2 = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=5, min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), param_grid = param_test2, scoring='roc_auc',n_jobs=4,iid=False, cv=5)gsearch2.fit(train[predictors],train[target])gsearch2.grid_scores_, gsearch2.best_params_, gsearch2.best_score_ ([mean: 0.84034, std: 0.00601, params: {&#39;max_depth&#39;: 4, &#39;min_child_weight&#39;: 4}, mean: 0.83921, std: 0.00658, params: {&#39;max_depth&#39;: 4, &#39;min_child_weight&#39;: 5}, mean: 0.84003, std: 0.00622, params: {&#39;max_depth&#39;: 4, &#39;min_child_weight&#39;: 6}, mean: 0.84071, std: 0.00553, params: {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 4}, mean: 0.83935, std: 0.00548, params: {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 5}, mean: 0.83871, std: 0.00492, params: {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 6}, mean: 0.83926, std: 0.00302, params: {&#39;max_depth&#39;: 6, &#39;min_child_weight&#39;: 4}, mean: 0.83717, std: 0.00483, params: {&#39;max_depth&#39;: 6, &#39;min_child_weight&#39;: 5}, mean: 0.83732, std: 0.00601, params: {&#39;max_depth&#39;: 6, &#39;min_child_weight&#39;: 6}], {&#39;max_depth&#39;: 5, &#39;min_child_weight&#39;: 4}, 0.8407073731353547) 至此，我们得到max_depth的理想取值为4，min_child_weight的理想取值为6。同时，我们还能看到cv的得分有了小小一点提高。需要注意的一点是，随着模型表现的提升，进一步提升的难度是指数级上升的，尤其是你的表现已经接近完美的时候。当然啦，你会发现，虽然min_child_weight的理想取值是6，但是我们还没尝试过大于6的取值。像下面这样，就可以尝试其它值。 1234567891011param_test2b = &#123; 'min_child_weight':[6,8,10,12] &#125;gsearch2b = GridSearchCV(estimator = XGBClassifier( learning_rate=0.1, n_estimators=140, max_depth=4, min_child_weight=2, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), param_grid = param_test2b, scoring='roc_auc',n_jobs=4,iid=False, cv=5)gsearch2b.fit(train[predictors],train[target])modelfit(gsearch2b.best_estimator_, train, predictors)gsearch2b.grid_scores_, gsearch2b.best_params_, gsearch2b.best_score_ Model Report Accuracy : 0.9854 AUC Score (Train): 0.875086 ([mean: 0.84003, std: 0.00622, params: {&#39;min_child_weight&#39;: 6}, mean: 0.83889, std: 0.00714, params: {&#39;min_child_weight&#39;: 8}, mean: 0.84004, std: 0.00661, params: {&#39;min_child_weight&#39;: 10}, mean: 0.83869, std: 0.00632, params: {&#39;min_child_weight&#39;: 12}], {&#39;min_child_weight&#39;: 10}, 0.840037896893036) png 我们可以看出，6确确实实是理想的取值了。 第三步：gamma参数调优 在已经调整好其它参数的基础上，我们可以进行gamma参数的调优了。Gamma参数取值范围可以很大，我这里把取值范围设置为5了。你其实也可以取更精确的gamma值。 1234567param_test3 = &#123; 'gamma':[i/10.0 for i in range(0,5)]&#125;gsearch3 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=140, max_depth=4, min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), param_grid = param_test3, scoring='roc_auc',n_jobs=4,iid=False, cv=5)gsearch3.fit(train[predictors],train[target])gsearch3.grid_scores_, gsearch3.best_params_, gsearch3.best_score_ ([mean: 0.84003, std: 0.00622, params: {&#39;gamma&#39;: 0.0}, mean: 0.84017, std: 0.00594, params: {&#39;gamma&#39;: 0.1}, mean: 0.83963, std: 0.00624, params: {&#39;gamma&#39;: 0.2}, mean: 0.83974, std: 0.00692, params: {&#39;gamma&#39;: 0.3}, mean: 0.83996, std: 0.00568, params: {&#39;gamma&#39;: 0.4}], {&#39;gamma&#39;: 0.1}, 0.8401672862430356) 从这里可以看出来，我们在第一步调参时设置的初始gamma值就是比较合适的。也就是说，理想的gamma值为0。在这个过程开始之前，最好重新调整boosting回合，因为参数都有变化。 从这里，可以看出，得分提高了。所以，最终得到的参数是： 12345678910111213xgb2 = XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=4, min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4,scale_pos_weight=1,seed=27)modelfit(xgb2, train, predictors) Model Report Accuracy : 0.9854 AUC Score (Train): 0.883777 png 调整subsample 和 colsample_bytree 参数 下一步是尝试不同的subsample 和 colsample_bytree 参数。我们分两个阶段来进行这个步骤。这两个步骤都取0.6,0.7,0.8,0.9作为起始值。 123456789param_test4 = &#123; 'subsample':[i/10.0 for i in range(6,10)], 'colsample_bytree':[i/10.0 for i in range(6,10)]&#125;gsearch4 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=3, min_child_weight=4, gamma=0.1, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), param_grid = param_test4, scoring='roc_auc',n_jobs=4,iid=False, cv=5)gsearch4.fit(train[predictors],train[target])gsearch4.grid_scores_, gsearch4.best_params_, gsearch4.best_score_ ([mean: 0.83836, std: 0.00840, params: {&#39;subsample&#39;: 0.6, &#39;colsample_bytree&#39;: 0.6}, mean: 0.83720, std: 0.00976, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.6}, mean: 0.83787, std: 0.00758, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.6}, mean: 0.83776, std: 0.00762, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.6}, mean: 0.83923, std: 0.01005, params: {&#39;subsample&#39;: 0.6, &#39;colsample_bytree&#39;: 0.7}, mean: 0.83800, std: 0.00853, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.7}, mean: 0.83819, std: 0.00779, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.7}, mean: 0.83925, std: 0.00906, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.7}, mean: 0.83977, std: 0.00831, params: {&#39;subsample&#39;: 0.6, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83867, std: 0.00870, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83879, std: 0.00797, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.8}, mean: 0.84144, std: 0.00854, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83878, std: 0.00760, params: {&#39;subsample&#39;: 0.6, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83922, std: 0.00823, params: {&#39;subsample&#39;: 0.7, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83912, std: 0.00765, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.9}, mean: 0.83926, std: 0.00843, params: {&#39;subsample&#39;: 0.9, &#39;colsample_bytree&#39;: 0.9}], {&#39;colsample_bytree&#39;: 0.8, &#39;subsample&#39;: 0.9}, 0.8414372201469303) 从这里可以看出来，subsample 和 colsample_bytree 参数的理想取值都是0.8。现在，我们以0.05为步长，在这个值附近尝试取值。 12345678910param_test5 = &#123; 'subsample':[i/100.0 for i in range(75,90,5)], 'colsample_bytree':[i/100.0 for i in range(75,90,5)]&#125;gsearch5 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4, min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), param_grid = param_test5, scoring='roc_auc',n_jobs=4,iid=False, cv=5)gsearch5.fit(train[predictors],train[target])gsearch5.grid_scores_, gsearch4.best_params_, gsearch4.best_score_ ([mean: 0.83922, std: 0.00800, params: {&#39;subsample&#39;: 0.75, &#39;colsample_bytree&#39;: 0.75}, mean: 0.84068, std: 0.00664, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.75}, mean: 0.84012, std: 0.00744, params: {&#39;subsample&#39;: 0.85, &#39;colsample_bytree&#39;: 0.75}, mean: 0.83893, std: 0.00756, params: {&#39;subsample&#39;: 0.75, &#39;colsample_bytree&#39;: 0.8}, mean: 0.84070, std: 0.00665, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.8}, mean: 0.84030, std: 0.00663, params: {&#39;subsample&#39;: 0.85, &#39;colsample_bytree&#39;: 0.8}, mean: 0.83961, std: 0.00628, params: {&#39;subsample&#39;: 0.75, &#39;colsample_bytree&#39;: 0.85}, mean: 0.83964, std: 0.00494, params: {&#39;subsample&#39;: 0.8, &#39;colsample_bytree&#39;: 0.85}, mean: 0.84064, std: 0.00733, params: {&#39;subsample&#39;: 0.85, &#39;colsample_bytree&#39;: 0.85}], {&#39;colsample_bytree&#39;: 0.8, &#39;subsample&#39;: 0.9}, 0.8414372201469303) 我们得到的理想取值还是原来的值。因此，最终的理想取值是: subsample: 0.8 colsample_bytree: 0.8 第五步：正则化参数调优 下一步是应用正则化来降低过拟合。由于gamma函数提供了一种更加有效地降低过拟合的方法，大部分人很少会用到这个参数。但是我们在这里也可以尝试用一下这个参数。我会在这里调整’reg_alpha’参数，然后’reg_lambda’参数留给你来完成。 123456789param_test6 = &#123; 'reg_alpha':[1e-5, 1e-2, 0.1, 1, 100]&#125;gsearch6 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4, min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), param_grid = param_test6, scoring='roc_auc',n_jobs=4,iid=False, cv=5)gsearch6.fit(train[predictors],train[target])gsearch6.grid_scores_, gsearch6.best_params_, gsearch6.best_score_ ([mean: 0.83949, std: 0.00720, params: {&#39;reg_alpha&#39;: 1e-05}, mean: 0.83940, std: 0.00607, params: {&#39;reg_alpha&#39;: 0.01}, mean: 0.84005, std: 0.00638, params: {&#39;reg_alpha&#39;: 0.1}, mean: 0.84062, std: 0.00775, params: {&#39;reg_alpha&#39;: 1}, mean: 0.81217, std: 0.01559, params: {&#39;reg_alpha&#39;: 100}], {&#39;reg_alpha&#39;: 1}, 0.8406243437179736) 我们可以看到，相比之前的结果，CV的得分甚至还降低了。但是我们之前使用的取值是十分粗糙的，我们在这里选取一个比较靠近理想值(0.01)的取值，来看看是否有更好的表现。 1234567param_test7 = &#123; 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]&#125;gsearch7 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=177, max_depth=4, min_child_weight=6, gamma=0.1, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread=4, scale_pos_weight=1,seed=27), param_grid = param_test7, scoring='roc_auc',n_jobs=4,iid=False, cv=5)gsearch7.fit(train[predictors],train[target])gsearch7.grid_scores_, gsearch7.best_params_, gsearch7.best_score_ ([mean: 0.83949, std: 0.00720, params: {&#39;reg_alpha&#39;: 0}, mean: 0.83949, std: 0.00720, params: {&#39;reg_alpha&#39;: 0.001}, mean: 0.83999, std: 0.00658, params: {&#39;reg_alpha&#39;: 0.005}, mean: 0.83940, std: 0.00607, params: {&#39;reg_alpha&#39;: 0.01}, mean: 0.83945, std: 0.00693, params: {&#39;reg_alpha&#39;: 0.05}], {&#39;reg_alpha&#39;: 0.005}, 0.8399870466856136) 可以看到，CV的得分提高了。现在，我们在模型中来使用正则化参数，来看看这个参数的影响。 1234567891011121314xgb3 = XGBClassifier( learning_rate =0.1, n_estimators=1000, max_depth=4, min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.005, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)modelfit(xgb3, train, predictors) Model Report Accuracy : 0.9854 AUC Score (Train): 0.883621 png 然后我们发现性能有了小幅度提高。 第6步：降低学习速率 最后，我们使用较低的学习速率，以及使用更多的决策树。我们可以用XGBoost中的CV函数来进行这一步工作。 1234567891011121314xgb4 = XGBClassifier( learning_rate =0.01, n_estimators=5000, max_depth=4, min_child_weight=6, gamma=0, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.005, objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)modelfit(xgb4, train, predictors) Model Report Accuracy : 0.9854 AUC Score (Train): 0.884030 png 至此，你可以看到模型的表现有了大幅提升，调整每个参数带来的影响也更加清楚了。 在文章的末尾，我想分享两个重要的思想： 1. 仅仅靠参数的调整和模型的小幅优化，想要让模型的表现有个大幅度提升是不可能的。GBM的最高得分是0.8487，XGBoost的最高得分是0.8494。确实是有一定的提升，但是没有达到质的飞跃。 2. 要想让模型的表现有一个质的飞跃，需要依靠其他的手段，诸如，特征工程(feature egineering) ，模型组合(ensemble of model),以及堆叠(stacking)等。 12","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]},{"title":"GBDT 算法原理","slug":"GBDT 算法原理","date":"2017-09-01T03:05:10.870Z","updated":"2017-09-01T02:54:18.000Z","comments":true,"path":"2017/09/01/GBDT 算法原理/","link":"","permalink":"http://yoursite.com/2017/09/01/GBDT 算法原理/","excerpt":"","text":"GBDT 算法原理 泰勒公式介绍 公式 \\[f(x)=\\sum_{n=0}^{\\infty}\\frac{f^{(n)}(x_{0})}{n!}(x-x_{0})^{n}\\] 一阶泰勒展开： \\[f(x)\\approx f(x_{0})+{f}&#39;(x_{0})(x-x_{0})\\] 二阶泰勒展开： \\[f(x)\\approx f(x_{0})+{f}&#39;(x_{0})(x-x_{0})+{f}&#39;&#39;(x_{0})\\frac{(x-x_{0})^2}{2}\\] 迭代形式：假设 \\(x^{t}=x^{t-1}+\\Delta x\\) 将 \\(f(t^{t})在x^{t-1}\\) 处进行泰勒展开 \\[\\begin{align*} f(x^{t})&amp;=f(x^{t-1}+\\Delta x) \\\\ &amp;\\approx f(x^{t-1})+{f}&#39;(x^{t-1})\\Delta x+{f}&#39;&#39;(x^{t-1})\\frac{\\Delta x^{2}}{2} \\end{align*}\\] 梯度下降法介绍 在机器学习任务中，需要最小化随时函数 \\(L(\\theta)\\),其中 \\(\\theta\\) 是要求解的模型参数。梯度下降法常用来求解这种无约束最优化问题，它是一种迭代方法:选取初值\\(\\theta^0\\)，不断迭代，更新\\(\\theta\\)的值，进行损失函数的极小化。 - 迭代公式： \\(\\theta^{t}=\\theta^{t-1}+\\Delta\\theta\\) - 将 \\(L(\\theta^{t})\\) 在\\(\\theta^{t-1}\\)处进行一阶泰勒展开： \\[\\begin{align*} L(\\theta^t)&amp;=L(\\theta^{t-1}+\\Delta\\theta)\\\\ &amp;\\approx L(\\theta^{t-1})+{L}&#39;(\\theta^{t-1})\\Delta\\theta \\end{align*}\\] 要使得\\(L(\\theta^{t})\\)&lt;\\(L(\\theta^{t-1})\\),可取：\\(\\Delta\\theta=-\\alpha{L}’(\\theta^{t-1})\\)，则：\\(\\theta^t=\\theta^{t-1}-\\alpha{L}&#39;(\\theta^{t-1})\\) 这里\\(\\alpha\\)是步长，可以通过line search 确定，但是一般直接赋一直小的数。 ## 牛顿法 将\\(L(\\theta^t)\\) 在\\(\\theta^{t-1}\\)处进行二阶泰勒展开： \\[L(\\theta^t)\\approx L(\\theta^{t-1})+{L}&#39;(\\theta^{t-1})\\Delta\\theta+{L&#39;&#39;}(\\theta^{t-1})\\frac{\\Delta\\theta^2}{2}\\] 为了简化分析过程，假设参数是标量（即\\(\\theta\\)只有一维），则可将一阶和二阶导数分别记为\\(g\\)和\\(h\\)： \\[L(\\theta^t)\\approx L(\\theta^{t-1})+g\\Delta\\theta+h\\frac{\\Delta\\theta^2}{2}\\] 要使得\\(L(\\theta^t)\\)极小，即让\\(g\\)和\\(h\\): \\[L(\\theta^t)\\approx L(\\theta^{t-1})+g\\Delta\\theta+h\\frac{\\Delta\\theta^2}{2}\\] 要使得\\(L(\\theta)\\)极小，即让 \\(g\\Delta\\theta+h\\frac{\\theta^{2}}{2}\\) 极小，可令: \\[\\frac{\\partial(g\\Delta\\theta+h\\frac{\\Delta\\theta^2}{2}) }{\\partial \\Delta\\theta}=0\\] 求得 \\(\\Delta\\theta = -\\frac{g}{h}\\)，故 \\(\\theta^{t}=\\theta^{t-1}+\\Delta\\theta=\\theta^{t-1}-\\frac{g}{h}\\) 参数\\(\\theta 推广到向量形式，迭代公式：\\theta^t=\\theta^{t-1}-H^{-1}g\\) 这里\\(H\\) 是海森矩阵 从参数空间到函数空间 GBDT 在函数空间中利用梯度下降法进行优化 XGBoost 在函数空间中利用牛顿法进优化 从Gradient Decend 到Gradient Boosting 参数空间 \\[\\theta^t(第t次迭代后的参数)=\\theta^{t-1}(第t-1次迭代后的参数)+\\theta_t(第t次迭代的参数增量)\\] 参数更新方向后负梯度方向 \\[\\theta_t=-a_{t}g_{t}\\] 最终参数等于每次迭代的增量的累加和 \\[\\theta = \\sum_{t=0}^{T}\\theta^t\\] \\(\\theta_0为初值\\) 函数空间 \\[f^t(x) (第t次迭代后的函数)=f^{t-1}(x)(第t-1次迭代后的函数)+f_t(x)(第t次迭代的函数增量)\\] - 同样地，拟合负梯度 \\[f_t(x)=-a_tg_t(x)\\] - 最终函数等于每次迭代的增量的累加和 \\[F(x)=\\sum_{t=0}^{T}f_t(x)\\] - \\(f_0(x)为模型初始值，通常为常数\\) 从Newton’s Method 到 Newton Boosting 参数空间 \\[\\theta^t(第t次迭代后的参数)=\\theta^{t-1}(第t-1次迭代后的参数)+\\theta_t(第t次迭代的参数增量)\\] 与梯度下降法唯一不同的就是参数增量 \\[\\theta_t=-H_{t}^{-1}g_{t}\\] 最终参数等于每次迭代的增量的累加和 \\[\\theta = \\sum_{t=0}^{T}\\theta^t\\] \\(\\theta_0为初值\\) 函数空间 \\[f^t(x) (第t次迭代后的函数)=f^{t-1}(x)(第t-1次迭代后的函数)+f_t(x)(第t次迭代的函数增量)\\] - 同样地，拟合负梯度 \\[f_t(x)=- \\frac {g_t(x)}{h_t(x)}\\] - 最终函数等于每次迭代的增量的累加和 \\[F(x)=\\sum_{t=0}^{T}f_t(x)\\] - \\(f_0(x)为模型初始值，通常为常数\\) 小结 Boosting 算法是一种加法模型（additive training） \\[F(x)=\\sum_{t=0}^{T}f_t(x)\\] Gradient Boosting Tree 算法原理 其模型F 定义为加法模型： \\[F(X;w)=\\sum_{t=0}^{T}\\alpha_th_t(x;w_t)=\\sum_{t=0}^{T}f_t(x;w_t)\\] 其中，x为输入样本，h为分类回归树，w是分类回归树的参数，\\(\\alpha\\)是每课树的权重。 通过最小化损失函数求解最优模型： \\[F^*=\\arg \\min_{F}\\sum_{i=0}^{N}L(y_i,F(x_i;w))\\] NP 难问题 —&gt; 通过贪心算法，迭代求解局部最优化 输入：\\((x_i,y_i),T,L\\) 1. 初始化 \\(f_0\\) for t=1 to T do 2.1. 计算相应: \\[y_i=-\\left [ \\frac{\\partial L(y_i,F(x_i)) }{\\partial F(x_i) } \\right ]_{F(x)=F_{t-1}(x)},i=1,2,...N\\] 2.2. 学习第t棵树: \\[w^* = \\arg\\min_{t}\\sum_{i=1}^{N}(y_i-h_t(x_i;w))^2\\] 2.3. line search 找步长:\\[\\rho^* = \\arg\\min_{\\rho}\\sum_{i=1}^{N}L(y_i,F_{t-1}(x_i)+\\rho h_{t}(x_i;w^*))\\] 2.4. 令\\[f_t=\\rho^*h_t(x;w^*)\\]更新模型：\\(F_t= F_{t-1}+f_t\\) 输出\\(F_T\\) GBDT 例子 本文将从一个简单的示例开始。我们希望根据一个人是否玩视频游戏、是否喜欢园艺以及戴帽子时的特点来判断这个人的年龄。我们总共有九个训练样本来构建模型。 梯度下降 接下来，让我们使用梯度下降的概念把这个想法形式化。假设我们有一个希望最小化的可微函数。例如 \\[L(x_1,x_2)=\\frac{1}{2}(x_1-15)^2+\\frac{1}{2}(x_2-25)^2\\] 我们的目标是找到一组能够最小化L的(x1, x2)。注意，这个函数可以理解为给定两个预测值x1和x2，计算其与两个数据点15和25的均方误差（使用1/2乘子更有利于梯度的计算）。尽管我们可以直接最小化这个函数，但是在面对更复杂的损失函数时，我们往往无法直接最小化损失函数，而梯度下降则为我们提供了一种可行的方式。 初始阶段： - 迭代次数M=100 - 迭代起始点s0=(0,0) - 迭代步长γ=0.1 在第m次迭代过程中（m取值从1到M）： - 计算函数L在数据点\\(s^{(m-1)}\\)处的梯度 - 沿梯度最大方向（梯度的反方向）前进步长γ的距离。公式如下： \\[s^m=s^{(m-1)}-\\gamma\\Delta L(s^{(m-1)})\\] 如果\\(\\gamma\\)很小且\\(M\\)足够大，那么\\(s^{M}\\)将落在函数\\(L\\)的最小值处 应用梯度下降 现在，我们可以在前面的梯度提升模型中使用梯度下降了。我们需要最小化的目标函数是L。我们的起始点是F0(x)。在模型第一次迭代m=1时，我们计算L关于F0(x)的梯度。然后，我们将一个弱分类器调整为梯度分量。在回归树的例子中，叶子节点会在具有相似特征的样本间计算梯度均值。对于每一个叶子节点，我们沿梯度均值的方向更新参数（使用线性搜索来确定步长的大小）。更新后的结果记为F1。随后，我们反复重复这个过程，直到得到FM。 我们修改了前面提到的梯度提升算法，使其适用于任何可微损失函数。 现在，让我们忘记前面的想法，从另外一个角度重新考虑我们的梯度提升模型。 使用常量来初始化模型： \\[F_0(X)=\\arg\\min_{\\gamma}\\sum_{i=1}^{n}L(y_i,\\gamma)\\] m从1到M： 计算伪残差 根据伪残差拟合基本学习器\\(h_m(x)\\) 计算步长乘子\\(\\gamma_m\\)（在树形模型的情况下，需要为每一个叶子节点计算不同的\\(\\gamma_m\\)） 更新\\(F_m(x)=F_{m-1}(x)+\\gamma_mh_m(x)\\) 如果你想要检测自己对梯度提升的理解是否正确，我们目前的梯度提升方法应用于示例问题的均方误差和绝对误差结果如下： Newton Boosting Tree 算法原理：详解XGBoost 模型函数形式 给定数据集\\(D = \\{(x_i,y_i)\\}\\)，XGBoost进行additive training，学习K棵树，采用以下函数对样本进行预测： \\[\\hat{y_i}= \\phi(x_i)=\\sum_{k=1}^{K}f_k(x_i), f_k\\in F\\] 这里\\(F\\)是假设空间，\\(f(x)\\)是回归树(CART): \\[F=\\{f(x)=w_{q(x)}\\}(q:\\mathbb{R}^{m}\\rightarrow T,w\\in \\mathbb{R}^T)\\] \\(q(x)\\)表示将样本x分到了某个叶子节点上，\\(w\\)是叶子节点的分数（leaf score）,所以\\(w_q(x)\\)表示回归树对样本的预测值 例子：预测一个人是否喜欢电脑游戏 回归树的预测输出是实数分数，可以用于回归，分类，排序等任务中。对于回归问题，可以直接作为目标值，对于分类问题，需要映射成概率，比如采用逻辑函数\\(\\sigma(z)=\\frac{1}{1+e^{-z}}\\) 目标函数 参数空间中的目标函数： \\[Obj(\\Theta) = L(\\Theta)(误差函数)+\\Omega(\\Theta)(正则化项)\\] 误差函数可以是square loss,logloss等，正则项可以是L1正则，L2正则等。 Ridge Regression(岭回归):\\[\\sum_{i=1}^{n}(y_i-\\theta^Tx_i)^2+\\lambda\\left \\|\\theta \\right \\|^2\\] LASSO :\\[\\sum_{i=1}^{n}(y_i-\\theta^Tx_i)^2+\\lambda\\left \\|\\theta \\right \\|_1\\] 正则项 正则项的的作用，可以从几个角度去解释： 通过偏方差分解去解释 PAC-learning 泛化界解释 Bayes 先验解释，把正则当成先验 从Bayes 角度来看，正则相当于对模型参数引入先验分布： L2正则，模型参数服从高数分布\\(\\theta\\sim N(0,\\sigma ^2)\\)对参数加了分布约束，大部分绝对值很小 L1 正则，模型参数服从拉普拉斯分布对参数界了分布约束，大部分取值为0 XGBoost 的目标函数(函数空间) \\[L(\\phi) = \\sum_{i}l(\\hat(y_i),y_i)+\\sum_{k}\\Omega(f_k)\\] 正则项对每棵回归树的复杂度进行了惩罚 相比原始的GBDT,XGBoost 的目标函数多了正则项，使得学习出来的模型更加不容易过拟合 又哪些指标可以衡量树的复杂度？ 树的深度，内部节点的个数，叶子节点的个数（T）,叶节点分数（w）。。。 XGBoost 采用的： \\[\\Omega(f)=\\gamma T+\\frac{1}{2}\\lambda \\left \\| \\omega \\right \\|^2\\] 对叶子节点个数进行惩罚，相当于再训练过程中做了剪枝 误差函数的二阶泰勒展开 第t次迭代后，模型的预测等于前t-1次的模型预测加上第t棵树的预测： \\[\\hat{y_i}^{(t)}=\\hat{y_i}^{(t-1)}+f_t(x_i)\\] 此时目标函数可写作： \\[L^{(t)}=\\sum_{i=1}^{n}l(y_i,\\hat{y_i}^{(t-1)}+f_t(x_i))+\\Omega(f_t)\\] 公式中 \\(y_i\\),\\(\\tilde{y_i}^{(t-1)}\\)都已知，模型要学习的只有第t棵树\\(f_t\\) 将误差函数再\\(\\tilde{y_i}^{(t-1)}\\)处进行二次泰勒展开： \\[L^{(t)}\\simeq \\sum_{i=1}^{n}[l(y_i,\\hat{y}^{(t-1)})+g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)\\] 公式中: \\(g_i=\\partial_{\\hat{y}^{(t-1)}}l(y_i,\\hat{y}^{(t-1)})\\) \\(h_i=\\partial_{\\hat{y}^{(t-1)}}^{2}l(y_i,\\hat{y}^{(t-1)})\\) 将公式中的常数项去掉，得到: \\[\\tilde{L}^{(t)}=\\sum_{i=1}^{n}[g_if_t(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)\\] 把\\(f_t,\\Omega(f_t)\\)写成树结构的形式，即把下式代入目标函数中 \\(f(x)=\\omega _{q(x)}\\) \\(\\Omega(f)=\\gamma T+\\frac{1}{2}\\lambda\\left \\|\\omega \\right \\|^2\\) 得到： \\[\\begin{align*} L^{(t)}&amp;=\\sum_{i=1}^{n}[g_if_i(x_i)+\\frac{1}{2}h_if_t^2(x_i)]+\\Omega(f_t)\\\\ &amp;=\\sum_{i=1}^{n}[g_iw_{q(x_i)}+\\frac{1}{2}h_iw^2_q(x_i)](对样本累加)+\\gamma T+\\lambda \\frac{1}{2}\\sum_{j=1}^{T}w_j^2(对叶节点累加) \\end{align*}\\] 怎么统一起来？ 定义每个叶节点j上的样本集合为 $I_j = {i q(x_i)=j } $ 则目标函数可以写成按叶节点累加的形式: \\[\\begin{align*} L^{(t)}&amp;=\\sum_{j=1}^{T}\\bigg[(\\sum_{i\\in I_j}g_i)w_{i}+\\frac{1}{2}(\\sum_{i\\in I_j}h_i+\\lambda)w^2_j\\bigg]+\\gamma T\\\\ &amp;=\\sum_{j=1}^{T}\\bigg[(G_jw_{i}+\\frac{1}{2}(H_j+\\lambda)w^2_j\\bigg]+\\gamma T \\end{align*}\\] 如果确定了树的结构（即q(x)确定），为了使目标函数最小，可以令其导数为0，解得每个叶节点得最优预测分数为： \\[w^*_j=-\\frac{G_j}{H_j+\\lambda}\\] 代入目标函数，得到最小损失为： \\[L^*=-\\frac{1}{2}\\sum_{j=1}^{T}\\frac{G^2_j}{H_j+\\lambda}+\\gamma T\\] 回归树得学习策略 当回归树得结构确定时，我们前面已经推导出其最优得叶节点分数以及对应得最小损失值，问题是怎么确定树的结构？ 暴力枚举所以可能的树结构，选择损失值最小的 -NP难问题。 贪心法，每次尝试分裂一个叶节点，计算分裂前后的增益，选择增益最大的。 分裂前后的增益怎么计算 ID3算法采用信息增益 C4.5算法采用信息增益比 CART 采用Gini 系数 XGBoost 呢？ XGBoost 的打分函数 \\[L^*=-\\frac{1}{2}\\sum_{j=1}^{T}\\color{Red}{\\frac{G_j^2}{H_j+\\lambda}}+\\gamma T\\] 标红部分衡量了每个叶子节点对总体损失的贡献，我们希望损失越小越好，则标红部分的值越大越好。 因此，对一个叶子节点进行分裂，分裂前后的增益定义为： \\[Gain = \\frac{G_L^2}{H_L+\\lambda}+\\frac{G_R^2}{H_R+\\lambda}-\\frac{(G_R+G_L)^2}{H_R+H_L+\\lambda}-\\gamma\\] Gain 的值越大，分裂后L减少越多。所以当对一个叶节点分割时，计算所又候选对应的gain,选取gain最大的进行分割 树节点分裂方法（Split Finding） 暴力枚举 遍历所有特征的所有可能的分割点，计算gain值，选取值最大的（feature，value）去分割 近似方法 对于每个特征，只考察分位点，减少计算复杂度 举例：三分位数 实际上XGBoost 不是简单地按照样本个数进行分位，而是以二阶导数值作为权重，比如： 为什么用\\(h_i\\)加权？ 把目标函数整理成以下形式，可以看出\\(h_i\\)又对loss加权作用 \\[\\sum_{i=1}^{n}\\frac{1}{2}h_i(f_t(x_i)-\\frac{g_i}{h_i})^2+\\Omega(f_t)+constant\\] 缺失值处理 当特征出现却失值时，XGBoost可以学习处默认的节点分裂方向 XGBoost的其他特性 行抽样（row sample） 列抽样（column sample） 借鉴随机森林 Shrinkage(缩减)，即学习速率 将学习速率调小，迭代次数增多，有正则化作用 支持自定义损失函数（需二阶可导）","categories":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/categories/机器学习/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"http://yoursite.com/tags/机器学习/"}]}]}